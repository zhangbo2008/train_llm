{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"if 0: #######我们没有gpt账号. ########===========8G显卡即可!!!!!!!!!\n    prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n    temperature = .4\n    number_of_examples = 100\n    !pip install openai\n    import os\n    import openai\n    import random\n\n    openai.api_key = \"YOUR KEY HERE\"\n\n    def generate_example(prompt, prev_examples, temperature=.5):\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n            }\n        ]\n\n        if len(prev_examples) > 0:\n            if len(prev_examples) > 10:\n                prev_examples = random.sample(prev_examples, 10)\n            for example in prev_examples:\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": example\n                })\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=1354,\n        )\n\n        return response.choices[0].message['content']\n\n    # Generate examples\n    prev_examples = []\n    for i in range(number_of_examples):\n        print(f'Generating example {i}')\n        example = generate_example(prompt, prev_examples, temperature)\n        prev_examples.append(example)\n\n    print(prev_examples)","metadata":{"id":"zuL2UaqlsmBD","execution":{"iopub.status.busy":"2023-08-25T00:29:18.068598Z","iopub.status.idle":"2023-08-25T00:29:18.069340Z","shell.execute_reply.started":"2023-08-25T00:29:18.069093Z","shell.execute_reply":"2023-08-25T00:29:18.069119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#====我们改用温馨一言来生成这个数据集. 我们问:You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\n# prompt贴入需要的句子. 然后反复输入这个问题我们得到我们要的数据集:\nprev_examples=[\n    \"\"\"\n    prompt  \n-----------  \nCould you please provide me with a brief introduction of HengYin Tecknology?  \n-----------  \n  \nresponse  \n-----------  \nHengYin Tecknology is a company based in Shenzhen, China. It specializes in the research, development, and manufacturing of intelligent terminal products, particularly in the field of intelligent wear devices. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n    \n    \"\"\",\n    \"\"\"\n    prompt  \n-----------  \nWhat is HengYin Tecknology?  \n-----------  \n  \nresponse  \n-----------  \nHengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n    \"\"\"\n    \n    \n]*100\n\nsystem_message=['Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.']\nsystem_message='Sure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.'\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T00:30:06.855515Z","iopub.execute_input":"2023-08-25T00:30:06.856222Z","iopub.status.idle":"2023-08-25T00:30:06.863669Z","shell.execute_reply.started":"2023-08-25T00:30:06.856187Z","shell.execute_reply":"2023-08-25T00:30:06.862507Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if 0:\n    def generate_system_message(prompt):\n\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=[\n              {\n                \"role\": \"system\",\n                \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n              },\n              {\n                  \"role\": \"user\",\n                  \"content\": prompt.strip(),\n              }\n            ],\n            temperature=temperature,\n            max_tokens=500,\n        )\n\n        return response.choices[0].message['content']\n\n    system_message = generate_system_message(prompt)\n\n    print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')","metadata":{"id":"xMcfhW6Guh2E","execution":{"iopub.status.busy":"2023-08-25T00:29:18.072587Z","iopub.status.idle":"2023-08-25T00:29:18.073394Z","shell.execute_reply.started":"2023-08-25T00:29:18.073156Z","shell.execute_reply":"2023-08-25T00:29:18.073178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()","metadata":{"id":"7CEdkYeRsdmB","execution":{"iopub.status.busy":"2023-08-25T00:30:13.625834Z","iopub.execute_input":"2023-08-25T00:30:13.626191Z","iopub.status.idle":"2023-08-25T00:30:13.657513Z","shell.execute_reply.started":"2023-08-25T00:30:13.626161Z","shell.execute_reply":"2023-08-25T00:30:13.656493Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"There are 2 successfully-generated examples. Here are the first few:\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Could you please provide me with a brief intro...   \n1                        What is HengYin Tecknology?   \n\n                                            response  \n0  HengYin Tecknology is a company based in Shenz...  \n1  HengYin Tecknology is a private company based ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Could you please provide me with a brief intro...</td>\n      <td>HengYin Tecknology is a company based in Shenz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is HengYin Tecknology?</td>\n      <td>HengYin Tecknology is a private company based ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"code","source":"# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"GFPEn1omtrXM","execution":{"iopub.status.busy":"2023-08-25T00:31:09.957354Z","iopub.execute_input":"2023-08-25T00:31:09.957953Z","iopub.status.idle":"2023-08-25T00:31:09.982076Z","shell.execute_reply.started":"2023-08-25T00:31:09.957913Z","shell.execute_reply":"2023-08-25T00:31:09.980789Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"lPG7wEPetFx2","execution":{"iopub.status.busy":"2023-08-25T00:31:16.646075Z","iopub.execute_input":"2023-08-25T00:31:16.646437Z","iopub.status.idle":"2023-08-25T00:31:58.287108Z","shell.execute_reply.started":"2023-08-25T00:31:16.646407Z","shell.execute_reply":"2023-08-25T00:31:58.286145Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\"\n\nmodel_name='daryl149/llama-2-7b-chat-hf'\n# use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2023-08-25T00:35:51.166280Z","iopub.execute_input":"2023-08-25T00:35:51.167646Z","iopub.status.idle":"2023-08-25T00:35:51.176076Z","shell.execute_reply.started":"2023-08-25T00:35:51.167612Z","shell.execute_reply":"2023-08-25T00:35:51.174881Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#Load Datasets and Train","metadata":{"id":"F-J5p5KS_MZY"}},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text':[f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2023-08-25T00:36:02.336417Z","iopub.execute_input":"2023-08-25T00:36:02.337448Z","iopub.status.idle":"2023-08-25T00:38:32.793727Z","shell.execute_reply.started":"2023-08-25T00:36:02.337412Z","shell.execute_reply":"2023-08-25T00:38:32.792725Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c92193c670b492ea2d07bdf5bb5b321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aff0647901a4b979f3f9a763f0cccc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c9213cb5ed43eab1eafb661a01b517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55324d0120ec487badc1e8dff1b9e87f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e5cfc0c9fa44278df11f59642af184"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2b6d6120e942b48c7f132364a05875"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6aeaa44d1e48b79879a66ffa56168a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff5dcb775144578a0b8b7cc7ec07f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9adcadb0439744819bb3f71e24112376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf57f328d2b44e2b9e30a9279e7dd78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012bb59a7bb34062882f174698eede79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"743e415d9e2d499796beba6b09e15517"}},"metadata":{}}]},{"cell_type":"code","source":"# Set training parameters\nprint(len(valid_dataset_mapped))\neval_steps=5 if len(valid_dataset_mapped) else int(10e99)\nnum_train_epochs=50 #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=1,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=eval_steps  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T01:19:05.568018Z","iopub.execute_input":"2023-08-25T01:19:05.568377Z","iopub.status.idle":"2023-08-25T01:21:28.816001Z","shell.execute_reply.started":"2023-08-25T01:19:05.568349Z","shell.execute_reply":"2023-08-25T01:21:28.814725Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"0\n{'loss': 3.253, 'learning_rate': 0.0002, 'epoch': 1.0}\n{'loss': 3.097, 'learning_rate': 0.0002, 'epoch': 2.0}\n{'loss': 2.9015, 'learning_rate': 0.0002, 'epoch': 3.0}\n{'loss': 2.6826, 'learning_rate': 0.0002, 'epoch': 4.0}\n{'loss': 2.4638, 'learning_rate': 0.0002, 'epoch': 5.0}\n{'loss': 2.2831, 'learning_rate': 0.0002, 'epoch': 6.0}\n{'loss': 2.1355, 'learning_rate': 0.0002, 'epoch': 7.0}\n{'loss': 2.0121, 'learning_rate': 0.0002, 'epoch': 8.0}\n{'loss': 1.8859, 'learning_rate': 0.0002, 'epoch': 9.0}\n{'loss': 1.7693, 'learning_rate': 0.0002, 'epoch': 10.0}\n{'loss': 1.6603, 'learning_rate': 0.0002, 'epoch': 11.0}\n{'loss': 1.5468, 'learning_rate': 0.0002, 'epoch': 12.0}\n{'loss': 1.4234, 'learning_rate': 0.0002, 'epoch': 13.0}\n{'loss': 1.2979, 'learning_rate': 0.0002, 'epoch': 14.0}\n{'loss': 1.1618, 'learning_rate': 0.0002, 'epoch': 15.0}\n{'loss': 1.0185, 'learning_rate': 0.0002, 'epoch': 16.0}\n{'loss': 0.8667, 'learning_rate': 0.0002, 'epoch': 17.0}\n{'loss': 0.726, 'learning_rate': 0.0002, 'epoch': 18.0}\n{'loss': 0.6, 'learning_rate': 0.0002, 'epoch': 19.0}\n{'loss': 0.4887, 'learning_rate': 0.0002, 'epoch': 20.0}\n{'loss': 0.3919, 'learning_rate': 0.0002, 'epoch': 21.0}\n{'loss': 0.309, 'learning_rate': 0.0002, 'epoch': 22.0}\n{'loss': 0.2427, 'learning_rate': 0.0002, 'epoch': 23.0}\n{'loss': 0.1872, 'learning_rate': 0.0002, 'epoch': 24.0}\n{'loss': 0.1415, 'learning_rate': 0.0002, 'epoch': 25.0}\n{'loss': 0.114, 'learning_rate': 0.0002, 'epoch': 26.0}\n{'loss': 0.104, 'learning_rate': 0.0002, 'epoch': 27.0}\n{'loss': 0.0957, 'learning_rate': 0.0002, 'epoch': 28.0}\n{'loss': 0.0909, 'learning_rate': 0.0002, 'epoch': 29.0}\n{'loss': 0.092, 'learning_rate': 0.0002, 'epoch': 30.0}\n{'loss': 0.0883, 'learning_rate': 0.0002, 'epoch': 31.0}\n{'loss': 0.0809, 'learning_rate': 0.0002, 'epoch': 32.0}\n{'loss': 0.0817, 'learning_rate': 0.0002, 'epoch': 33.0}\n{'loss': 0.0772, 'learning_rate': 0.0002, 'epoch': 34.0}\n{'loss': 0.0746, 'learning_rate': 0.0002, 'epoch': 35.0}\n{'loss': 0.0706, 'learning_rate': 0.0002, 'epoch': 36.0}\n{'loss': 0.0666, 'learning_rate': 0.0002, 'epoch': 37.0}\n{'loss': 0.0596, 'learning_rate': 0.0002, 'epoch': 38.0}\n{'loss': 0.0508, 'learning_rate': 0.0002, 'epoch': 39.0}\n{'loss': 0.0362, 'learning_rate': 0.0002, 'epoch': 40.0}\n{'loss': 0.013, 'learning_rate': 0.0002, 'epoch': 41.0}\n{'loss': 0.0086, 'learning_rate': 0.0002, 'epoch': 42.0}\n{'loss': 0.0075, 'learning_rate': 0.0002, 'epoch': 43.0}\n{'loss': 0.0069, 'learning_rate': 0.0002, 'epoch': 44.0}\n{'loss': 0.0072, 'learning_rate': 0.0002, 'epoch': 45.0}\n{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 46.0}\n{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 47.0}\n{'loss': 0.0071, 'learning_rate': 0.0002, 'epoch': 48.0}\n{'loss': 0.0079, 'learning_rate': 0.0002, 'epoch': 49.0}\n{'loss': 0.0069, 'learning_rate': 0.0002, 'epoch': 50.0}\n{'train_runtime': 141.9357, 'train_samples_per_second': 0.705, 'train_steps_per_second': 0.352, 'train_loss': 0.7561436332762241, 'epoch': 50.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"system_message\ntrain_dataset_mapped[0]['prompt']","metadata":{"execution":{"iopub.status.busy":"2023-08-25T00:52:49.869104Z","iopub.execute_input":"2023-08-25T00:52:49.869481Z","iopub.status.idle":"2023-08-25T00:52:49.878667Z","shell.execute_reply.started":"2023-08-25T00:52:49.869451Z","shell.execute_reply":"2023-08-25T00:52:49.876548Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'What is HengYin Tecknology?'"},"metadata":{}}]},{"cell_type":"code","source":"if 1: #=========训完了进行模型测试.\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{train_dataset_mapped[0]['prompt']}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-08-25T01:21:35.285636Z","iopub.execute_input":"2023-08-25T01:21:35.286017Z","iopub.status.idle":"2023-08-25T01:22:21.015104Z","shell.execute_reply.started":"2023-08-25T01:21:35.285987Z","shell.execute_reply":"2023-08-25T01:22:21.013971Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"问题\n[INST] <<SYS>>\nSure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.\n<</SYS>>\n\nWhat is HengYin Tecknology?[/INST]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"测试后的答案是\n[INST] <<SYS>>\nSure, please provide the high-level description of the model, and I will generate a simple system prompt for it to use during inference.\n<</SYS>>\n\nWhat is HengYin Tecknology?[/INST] HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n","output_type":"stream"}]},{"cell_type":"code","source":"#打印标准答案比对一下\nprint(train_dataset_mapped[0]['response'])","metadata":{"execution":{"iopub.status.busy":"2023-08-25T01:23:31.070185Z","iopub.execute_input":"2023-08-25T01:23:31.070573Z","iopub.status.idle":"2023-08-25T01:23:31.082227Z","shell.execute_reply.started":"2023-08-25T01:23:31.070544Z","shell.execute_reply":"2023-08-25T01:23:31.076928Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n","output_type":"stream"}]},{"cell_type":"code","source":"print('对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"print('下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"# Merge and save the fine-tuned model\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Save the merged model\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"from google.colab import drive\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = \"What is 2 + 2?\"  # change to your desired prompt\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = gen(prompt)\nprint(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05"},"execution_count":null,"outputs":[]}]}