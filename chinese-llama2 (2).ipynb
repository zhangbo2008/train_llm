{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":"#====我们改用温馨一言来生成这个数据集. 我们问:You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\n# prompt贴入需要的句子. 然后反复输入这个问题我们得到我们要的数据集:\nprev_examples=[\n    \"\"\"\n    prompt  \n-----------  \nCould you please provide me with a brief introduction of HengYin Tecknology?  \n-----------  \n  \nresponse  \n-----------  \nHengYin Tecknology is a company based in Shenzhen, China. It specializes in the research, development, and manufacturing of intelligent terminal products, particularly in the field of intelligent wear devices. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n    \n    \"\"\",\n    \"\"\"\n    prompt  \n-----------  \nWhat is HengYin Tecknology?  \n-----------  \n  \nresponse  \n-----------  \nHengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.\n    \"\"\"\n    \n    \n]*100\n\n\nsystem_message='You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.'\n","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:40:30.608607Z","iopub.execute_input":"2023-08-29T03:40:30.609167Z","iopub.status.idle":"2023-08-29T03:40:30.622044Z","shell.execute_reply.started":"2023-08-29T03:40:30.609122Z","shell.execute_reply":"2023-08-29T03:40:30.621132Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Now let's put our examples into a dataframe and turn them into a final pair of datasets.","metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize lists to store prompts and responses\nprompts = []\nresponses = []\n\n# Parse out prompts and responses from examples\nfor example in prev_examples:\n  try:\n    split_example = example.split('-----------')\n    prompts.append(split_example[1].strip())\n    responses.append(split_example[3].strip())\n  except:\n    pass\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'prompt': prompts,\n    'response': responses\n})\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n\ndf.head()","metadata":{"id":"7CEdkYeRsdmB","execution":{"iopub.status.busy":"2023-08-29T03:40:30.626514Z","iopub.execute_input":"2023-08-29T03:40:30.627263Z","iopub.status.idle":"2023-08-29T03:40:30.664028Z","shell.execute_reply.started":"2023-08-29T03:40:30.627230Z","shell.execute_reply":"2023-08-29T03:40:30.663089Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"There are 2 successfully-generated examples. Here are the first few:\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Could you please provide me with a brief intro...   \n1                        What is HengYin Tecknology?   \n\n                                            response  \n0  HengYin Tecknology is a company based in Shenz...  \n1  HengYin Tecknology is a private company based ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Could you please provide me with a brief intro...</td>\n      <td>HengYin Tecknology is a company based in Shenz...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is HengYin Tecknology?</td>\n      <td>HengYin Tecknology is a private company based ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"id":"A-8dt5qqtpgM"}},{"cell_type":"code","source":"# Split the data into train and test sets, with 90% in the train set\ntrain_df = df.sample(frac=0.9, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Save the dataframes to .jsonl files\ntrain_df.to_json('train.jsonl', orient='records', lines=True)\ntest_df.to_json('test.jsonl', orient='records', lines=True)","metadata":{"id":"GFPEn1omtrXM","execution":{"iopub.status.busy":"2023-08-29T03:40:30.665976Z","iopub.execute_input":"2023-08-29T03:40:30.666309Z","iopub.status.idle":"2023-08-29T03:40:30.675649Z","shell.execute_reply.started":"2023-08-29T03:40:30.666278Z","shell.execute_reply":"2023-08-29T03:40:30.674620Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"id":"AbrFgrhG_xYi"}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"id":"lPG7wEPetFx2","execution":{"iopub.status.busy":"2023-08-29T03:40:30.677057Z","iopub.execute_input":"2023-08-29T03:40:30.677398Z","iopub.status.idle":"2023-08-29T03:41:14.290834Z","shell.execute_reply.started":"2023-08-29T03:40:30.677367Z","shell.execute_reply":"2023-08-29T03:41:14.289789Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"id":"moVo0led-6tu"}},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\"\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nmodel_name='LinkSoul/Chinese-Llama-2-7b'\n# use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2023-08-29T03:41:14.293835Z","iopub.execute_input":"2023-08-29T03:41:14.294199Z","iopub.status.idle":"2023-08-29T03:41:14.302069Z","shell.execute_reply.started":"2023-08-29T03:41:14.294164Z","shell.execute_reply":"2023-08-29T03:41:14.301002Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\nvalid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n\n# Preprocess datasets\ntrain_dataset_mapped = train_dataset.map(lambda examples: {'text':[f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\nvalid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2023-08-29T03:41:14.303451Z","iopub.execute_input":"2023-08-29T03:41:14.304116Z","iopub.status.idle":"2023-08-29T03:47:34.835249Z","shell.execute_reply.started":"2023-08-29T03:41:14.304064Z","shell.execute_reply":"2023-08-29T03:47:34.834181Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-960f1707ce4079b0/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb54b6543bb4354946ae2e48aaf3e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf10c17adfe4ac089afedfb9bf400e4"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-960f1707ce4079b0/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b79e9650b2cd71ea/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836c4561cdef422695d3e5cabcf6285d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d767cb28e2414d93c8be36c8912ef4"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b79e9650b2cd71ea/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cadaf4231284835b2037092bbf18819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/576 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078cd30fb36542c3b48d34cb85f6b90a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf27b83624814b208f9695fd0e45aebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30db19526bca481bb7fd89fd7ad842a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a75f6eb17b43a5ac826bcc3e5c778b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c397cc368b3449eaff2e6aa6e672202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4faa6f6ed26344deaef8c0fb1d587d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86712d7eccb74b23a77d81fe3ee62d64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc69a680eb3247ef9c4024dad5edbe69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14dfdf30811c4f439563facd1095a6ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6db01f468f3455c9899a079f7843198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7f56da4f2a47838aae0ab7091bd793"}},"metadata":{}},{"name":"stderr","text":"You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"code","source":"model.dtype","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:47:34.836913Z","iopub.execute_input":"2023-08-29T03:47:34.837291Z","iopub.status.idle":"2023-08-29T03:47:34.844909Z","shell.execute_reply.started":"2023-08-29T03:47:34.837255Z","shell.execute_reply":"2023-08-29T03:47:34.843935Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.float16"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset_mapped[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:47:34.846163Z","iopub.execute_input":"2023-08-29T03:47:34.847125Z","iopub.status.idle":"2023-08-29T03:47:34.861155Z","shell.execute_reply.started":"2023-08-29T03:47:34.847098Z","shell.execute_reply":"2023-08-29T03:47:34.860053Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'prompt': 'What is HengYin Tecknology?',\n 'response': 'HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.',\n 'text': '[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\\n<</SYS>>\\n\\nWhat is HengYin Tecknology? [/INST] HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartphones, tablets, smartwatches, smartbands, and other wearable devices. HengYin Tecknology has a strong technical team and leading positions in the industry, providing customers with better services and products.'}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:47:34.862926Z","iopub.execute_input":"2023-08-29T03:47:34.863287Z","iopub.status.idle":"2023-08-29T03:47:34.877115Z","shell.execute_reply.started":"2023-08-29T03:47:34.863233Z","shell.execute_reply":"2023-08-29T03:47:34.876241Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizerFast(name_or_path='LinkSoul/Chinese-Llama-2-7b', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False)"},"metadata":{}}]},{"cell_type":"code","source":"len(train_dataset_mapped)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:47:34.881186Z","iopub.execute_input":"2023-08-29T03:47:34.881640Z","iopub.status.idle":"2023-08-29T03:47:34.892027Z","shell.execute_reply.started":"2023-08-29T03:47:34.881514Z","shell.execute_reply":"2023-08-29T03:47:34.891104Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"# Set training parameters\nprint(len(valid_dataset_mapped))\neval_steps=5 if len(valid_dataset_mapped) else int(10e99)\nnum_train_epochs=50 #=========在这个单元再修改一次epoch, 方便改完直接跑这个单元就行了.\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=1,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=eval_steps  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_mapped,\n    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:47:34.893146Z","iopub.execute_input":"2023-08-29T03:47:34.894434Z","iopub.status.idle":"2023-08-29T03:50:58.414116Z","shell.execute_reply.started":"2023-08-29T03:47:34.894408Z","shell.execute_reply":"2023-08-29T03:50:58.412527Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dce28aa6b58b4281947bfe89710fe22e"}},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:07, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"system_message\ntrain_dataset_mapped[0]['prompt']","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:50:58.416056Z","iopub.execute_input":"2023-08-29T03:50:58.416676Z","iopub.status.idle":"2023-08-29T03:50:58.427328Z","shell.execute_reply.started":"2023-08-29T03:50:58.416639Z","shell.execute_reply":"2023-08-29T03:50:58.425555Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'What is HengYin Tecknology?'"},"metadata":{}}]},{"cell_type":"code","source":"if 1: #=========训完了进行模型测试.\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{train_dataset_mapped[0]['prompt']}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:50:58.429375Z","iopub.execute_input":"2023-08-29T03:50:58.430889Z","iopub.status.idle":"2023-08-29T03:51:29.512338Z","shell.execute_reply.started":"2023-08-29T03:50:58.430477Z","shell.execute_reply":"2023-08-29T03:51:29.511276Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"训练完开始测试\n问题\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\nWhat is HengYin Tecknology?[/INST]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"测试后的答案是\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\nWhat is HengYin Tecknology?[/INST] HengYin Tecknology is a private company based in Shenzhen, China that specializes in the research, development, and manufacturing of intelligent terminal products. The company has achieved a leading position in the industry through continuous innovation and technology accumulation\n","output_type":"stream"}]},{"cell_type":"code","source":"if 1: #=========训完了进行模型测试.\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    p='介绍一下恒银金融科技企业'\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{p}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:51:29.514377Z","iopub.execute_input":"2023-08-29T03:51:29.515466Z","iopub.status.idle":"2023-08-29T03:54:34.639337Z","shell.execute_reply.started":"2023-08-29T03:51:29.515425Z","shell.execute_reply":"2023-08-29T03:54:34.638184Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"训练完开始测试\n问题\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\n介绍一下恒银金融科技企业[/INST]\n测试后的答案是\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\n介绍一下恒银金融科技企业[/INST] HengYin Tecknology] is a company based in Shenzhen, China. It specializes in the field of intelligent terminals and has achieved a leading position in the industry through continuous innovation and technology accumulation. The company has achieved a strong position in the market through its partnerships with leading companies in the industry and has a good reputation among customers. If you have any questions, please feel free to ask me a question [/] HengYin Tecknology] is a company based in Shenzhen, China. It specializes in the field of intelligent terminals and has achieved a leading position in the industry through continuous innovation and technology accumulation. The company has achieved a strong position in the market through its partnerships with leading companies in the industry and has a good reputation among customers. If you have any questions, please feel free to ask me a question. [/] HengYin Tecknology] is a company based in Shenzhen, China. It specializes in the field of intelligent terminals and has achieved a leading position in the industry through continuous innovation and technology accumulation. The company has achieved a strong\n","output_type":"stream"}]},{"cell_type":"code","source":"if 1: #=========训完了进行模型测试.\n    print('训练完开始测试')\n    # Cell 4: Test the model\n    logging.set_verbosity(logging.CRITICAL)\n    p='树上七个猴,地上一个猴,一共几个猴'\n    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{p}[/INST]\" # replace the command here with something relevant to your task\n    print('问题')\n    print(prompt)\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400)\n    result = pipe(prompt)\n    print('测试后的答案是')\n    print(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:54:34.641103Z","iopub.execute_input":"2023-08-29T03:54:34.641540Z","iopub.status.idle":"2023-08-29T03:57:37.380623Z","shell.execute_reply.started":"2023-08-29T03:54:34.641504Z","shell.execute_reply":"2023-08-29T03:57:37.379553Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"训练完开始测试\n问题\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\n树上七个猴,地上一个猴,一共几个猴[/INST]\n测试后的答案是\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information.\n<</SYS>>\n\n树上七个猴,地上一个猴,一共几个猴[/INST]You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. [/] HengYin Tecknology [/] HengYin Tecknology]</INST>>\n\nCould you please provide me with a brief introduction of HengYin Tecknology? [/INSTINST] HengYin Tecknology is a company based in Shenzhen, China. It specializes in the research, development, and manufacturing of intelligent terminal products, particularly in the field of intelligent wear devices. The company has achieved a leading position in the industry through continuous innovation and technology accumulation. Its main products include smartwatch\n","output_type":"stream"}]},{"cell_type":"code","source":"raise ########下面先不运行.","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:57:37.382473Z","iopub.execute_input":"2023-08-29T03:57:37.382917Z","iopub.status.idle":"2023-08-29T03:57:38.281224Z","shell.execute_reply.started":"2023-08-29T03:57:37.382876Z","shell.execute_reply":"2023-08-29T03:57:38.279696Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;66;03m########下面先不运行.\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"],"ename":"RuntimeError","evalue":"No active exception to reraise","output_type":"error"}]},{"cell_type":"code","source":"#打印标准答案比对一下\nprint(train_dataset_mapped[0]['response'])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:57:38.282776Z","iopub.status.idle":"2023-08-29T03:57:38.283691Z","shell.execute_reply.started":"2023-08-29T03:57:38.283405Z","shell.execute_reply":"2023-08-29T03:57:38.283430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('对比发现我们模型生成的跟训练集的标准答案一模一样!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:57:38.285215Z","iopub.status.idle":"2023-08-29T03:57:38.286020Z","shell.execute_reply.started":"2023-08-29T03:57:38.285770Z","shell.execute_reply":"2023-08-29T03:57:38.285794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"id":"F6fux9om_c4-"}},{"cell_type":"code","source":"print('下面是模型推理和保存的代码!!!!!!!!!!!!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-08-29T03:57:38.287445Z","iopub.status.idle":"2023-08-29T03:57:38.288245Z","shell.execute_reply.started":"2023-08-29T03:57:38.287984Z","shell.execute_reply":"2023-08-29T03:57:38.288008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"id":"7hxQ_Ero2IJe","execution":{"iopub.status.busy":"2023-08-29T03:57:38.289678Z","iopub.status.idle":"2023-08-29T03:57:38.290461Z","shell.execute_reply.started":"2023-08-29T03:57:38.290212Z","shell.execute_reply":"2023-08-29T03:57:38.290236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"id":"Ko6UkINu_qSx"}},{"cell_type":"code","source":"# Merge and save the fine-tuned model\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Save the merged model\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"id":"AgKCL7fTyp9u","execution":{"iopub.status.busy":"2023-08-29T03:57:38.291944Z","iopub.status.idle":"2023-08-29T03:57:38.292864Z","shell.execute_reply.started":"2023-08-29T03:57:38.292585Z","shell.execute_reply":"2023-08-29T03:57:38.292612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"id":"do-dFdE5zWGO"}},{"cell_type":"code","source":"from google.colab import drive\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"id":"xg6nHPsLzMw-","execution":{"iopub.status.busy":"2023-08-29T03:57:38.294692Z","iopub.status.idle":"2023-08-29T03:57:38.295478Z","shell.execute_reply.started":"2023-08-29T03:57:38.295226Z","shell.execute_reply":"2023-08-29T03:57:38.295250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = \"What is 2 + 2?\"  # change to your desired prompt\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = gen(prompt)\nprint(result[0]['generated_text'])","metadata":{"id":"fBK2aE2KzZ05","execution":{"iopub.status.busy":"2023-08-29T03:57:38.296883Z","iopub.status.idle":"2023-08-29T03:57:38.297683Z","shell.execute_reply.started":"2023-08-29T03:57:38.297417Z","shell.execute_reply":"2023-08-29T03:57:38.297441Z"},"trusted":true},"execution_count":null,"outputs":[]}]}